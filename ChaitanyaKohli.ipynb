{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkDKimmhwmBz",
        "outputId": "ce37a387-4189-423b-8c78-04f9e8621137"
      },
      "outputs": [],
      "source": [
        "import PyPDF2\n",
        "from PyPDF2 import PdfReader\n",
        "import os\n",
        "from glob import glob\n",
        "import random as rand\n",
        "import langdetect\n",
        "import openai\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "openai.api_key=os.environ.get('OPENAI_API_KEY')\n",
        "from tenacity import (\n",
        "    retry,\n",
        "    stop_after_attempt,\n",
        "    wait_random_exponential,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZh3Yklq5tSE"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ddiHCJzV1GgU"
      },
      "outputs": [],
      "source": [
        "pdfs = glob('Publications/*.pdf')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttkd7qWl4FpR"
      },
      "source": [
        "### Checking wether the PDF is readable or not"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S73ZBWAZ1a28",
        "outputId": "dcf5bd0a-ac95-48e3-dd82-0b0d553bbcc6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Multiple definitions in dictionary at byte 0x1cc6b for key /MediaBox\n",
            "Multiple definitions in dictionary at byte 0x1ce61 for key /MediaBox\n",
            "Multiple definitions in dictionary at byte 0x1d014 for key /MediaBox\n",
            "Multiple definitions in dictionary at byte 0x1d1ae for key /MediaBox\n",
            "Multiple definitions in dictionary at byte 0x1d33d for key /MediaBox\n",
            "Multiple definitions in dictionary at byte 0x1d4af for key /MediaBox\n",
            "Multiple definitions in dictionary at byte 0x1d699 for key /MediaBox\n",
            "Multiple definitions in dictionary at byte 0x1d85b for key /MediaBox\n",
            "Multiple definitions in dictionary at byte 0x1db05 for key /MediaBox\n",
            "Multiple definitions in dictionary at byte 0x1cc6b for key /MediaBox\n",
            "Multiple definitions in dictionary at byte 0x1ce61 for key /MediaBox\n",
            "Multiple definitions in dictionary at byte 0x1d014 for key /MediaBox\n",
            "Multiple definitions in dictionary at byte 0x1d1ae for key /MediaBox\n",
            "Multiple definitions in dictionary at byte 0x1d33d for key /MediaBox\n",
            "Multiple definitions in dictionary at byte 0x1d4af for key /MediaBox\n",
            "Multiple definitions in dictionary at byte 0x1d699 for key /MediaBox\n",
            "Multiple definitions in dictionary at byte 0x1d85b for key /MediaBox\n",
            "Multiple definitions in dictionary at byte 0x1db05 for key /MediaBox\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Publications/15_Nazneen.pdf', 'Publications/Tariq_2019.pdf', 'Publications/Asd_Cry_patterns.pdf', 'Publications/Tariq2018.pdf']\n",
            "['Publications/Young_Behavior.pdf', 'Publications/1_RamÄ±rez-Duque_.pdf', 'Publications/Patten_Audio.pdf', 'Publications/zhao2020.pdf', 'Publications/carpenter2020 (1).pdf', 'Publications/LEE.pdf', 'Publications/Abbas_2020.pdf', 'Publications/Dawson.pdf', 'Publications/Qiu.pdf', 'Publications/22_Ouss_ASD.pdf', 'Publications/Abbas_2018.pdf']\n"
          ]
        }
      ],
      "source": [
        "unreadable_pdfs = []\n",
        "readable_pdfs = []\n",
        "for pdf in pdfs:\n",
        "  reader = PdfReader(pdf)\n",
        "  page = reader.pages[0]\n",
        "  text = page.extract_text()\n",
        "  if text == '':\n",
        "    unreadable_pdfs.append(pdf)\n",
        "  else:\n",
        "    readable_pdfs.append(pdf)\n",
        "    \n",
        "for pdf in readable_pdfs:\n",
        "  _pdf = PdfReader(pdf)\n",
        "  page = _pdf.pages[0]\n",
        "  text = page.extract_text()\n",
        "  if langdetect.detect(text)!='en':\n",
        "    unreadable_pdfs.append(pdf)\n",
        "    readable_pdfs.remove(pdf)\n",
        "\n",
        "print(unreadable_pdfs)\n",
        "print(readable_pdfs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "eTsuzVJ4L0zC",
        "outputId": "2c72ad23-99d0-4d40-bbee-45fbcf55d1cf"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import easyocr\n",
        "# import cv2\n",
        "# import numpy as np\n",
        "# import fitz\n",
        "\n",
        "# class OCR:\n",
        "#     def __init__(self, lang:str, gpu:bool=True):\n",
        "#         super().__init__()\n",
        "#         self.device = 'cpu' if not gpu else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#         if self.device == 'cpu' and gpu:\n",
        "#             print(\"GPU not available, using CPU instead\")\n",
        "#         self.lang = lang\n",
        "#         self.reader = easyocr.Reader([lang], gpu=True if self.device=='cuda' else False)\n",
        "\n",
        "#     def get_bbox(self, img)->np.array:\n",
        "#         \"\"\"Get bounding box of text in image format (x_min, x_max, y_min, y_max)\"\"\"\n",
        "#         bbox = np.array(self.reader.detect(img)[0][0])\n",
        "#         return bbox\n",
        "\n",
        "#     def read_img(self, img_path)->np.array:\n",
        "#         \"\"\"Read image and return image\"\"\"\n",
        "#         img = cv2.imread(img_path)\n",
        "#         img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "#         return img\n",
        "\n",
        "#     def get_text(self, img, detail=0)->str:\n",
        "#         \"\"\"return text from image\"\"\"\n",
        "#         text = self.reader.readtext(img, detail=detail)\n",
        "#         return text\n",
        "\n",
        "# class pdf2images:\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "\n",
        "#     def get_images_from_pdf(self, idx, pdf_path, output_folder:str=None, save_images_locally:bool=False):\n",
        "#         if save_images_locally:\n",
        "#             if output_folder is None:\n",
        "#                 raise ValueError(\"output_folder must be specified if save_images_locally is True\")\n",
        "#             if not os.path.exists(output_folder):\n",
        "#                 os.makedirs(output_folder)\n",
        "#         doc = fitz.open(pdf_path)\n",
        "#         count = idx\n",
        "#         j = 0\n",
        "#         for i in doc:\n",
        "#             images = i.get_pixmap()\n",
        "#             images.save(str(count)+'_'+str(j)+\".png\")\n",
        "#             j+=1\n",
        "#             count+=1\n",
        "#         return count\n",
        "\n",
        "#     def pdfs_to_images(self, pdf_paths, output_folder, save_images_locally:bool=False):\n",
        "#         idx=0\n",
        "#         for pdf_path in pdf_paths:\n",
        "#             idx = self.get_images_from_pdf(idx = idx, pdf_path = pdf_path, output_folder = output_folder, save_images_locally = save_images_locally)\n",
        "# pdf_image_gen = pdf2images()\n",
        "# ocr = OCR('en')\n",
        "# OUTPUT_PATH = '/content/drive/MyDrive/Publications_images'\n",
        "# pdf_image_gen.pdfs_to_images(unreadable_pdfs, OUTPUT_PATH, save_images_locally=True)\n",
        "# images_path = glob('/content/drive/MyDrive/Publications_images/*.png')\n",
        "# for image in images_path:\n",
        "#   img = ocr.read_img(image)\n",
        "#   text = ocr.get_text(img)\n",
        "#   print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "uVDR84iPbphq"
      },
      "outputs": [],
      "source": [
        "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
        "def generate_text(prompt:str, sys_prompt:str, temperature:float=0.01):\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": sys_prompt\n",
        "        },\n",
        "        {\n",
        "            'role':'user',\n",
        "            'content':prompt\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo-1106\",\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "        max_tokens=1000,\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message['content']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The input is New Text from research paper, and the Previous Text has been cleaned already. From the New Text remove the table data, the refrences, and the figure, while keeping the core of the text same. If possible summarise the table data, and what the figure was for.\n"
          ]
        }
      ],
      "source": [
        "system_prompt = \"\"\"The input is New Text from research paper, and the Previous Text has been cleaned already. From the New Text remove the table data, the refrences, and the figure, while keeping the core of the text same. If possible summarise the table data, and what the figure was for.\"\"\"\n",
        "print(system_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Multiple definitions in dictionary at byte 0x1cc6b for key /MediaBox\n",
            "Multiple definitions in dictionary at byte 0x1ce61 for key /MediaBox\n",
            "Multiple definitions in dictionary at byte 0x1d014 for key /MediaBox\n",
            "Multiple definitions in dictionary at byte 0x1d1ae for key /MediaBox\n",
            "Multiple definitions in dictionary at byte 0x1d33d for key /MediaBox\n",
            "Multiple definitions in dictionary at byte 0x1d4af for key /MediaBox\n",
            "Multiple definitions in dictionary at byte 0x1d699 for key /MediaBox\n",
            "Multiple definitions in dictionary at byte 0x1d85b for key /MediaBox\n",
            "Multiple definitions in dictionary at byte 0x1db05 for key /MediaBox\n"
          ]
        }
      ],
      "source": [
        "for pdf_path in readable_pdfs:\n",
        "  pdf = PdfReader(pdf_path)\n",
        "  cleaned_text = ''\n",
        "  for page in pdf.pages:\n",
        "    text = page.extract_text()\n",
        "    text = text.lower()\n",
        "    prompt = f\"\"\"Previous Text: {cleaned_text}\n",
        "    New Text{text}\"\"\"\n",
        "    cleaned_text = cleaned_text+generate_text(prompt, system_prompt, temperature=0.2)\n",
        "      \n",
        "  with open(os.path.join('Texts' ,f\"f{pdf_path.split('/')[-1].split('.')[0]}.txt\"), 'w') as f:\n",
        "    f.write(cleaned_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pymongo\n",
        "import openai\n",
        "import langchain\n",
        "import langchain_openai\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from pymongo import MongoClient\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "import pandas as pd\n",
        "from langchain.chains import question_answering\n",
        "from langchain.docstore.document import Document\n",
        "import numpy as np\n",
        "MONGO_CONNECTION_STRING = os.environ.get('MONGODB_ATLAS_CONNECTION_STRING')\n",
        "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "client = MongoClient(MONGO_CONNECTION_STRING)\n",
        "dbName = 'Publications'\n",
        "collectionName = 'Embedding_of_publications'\n",
        "collection = client[dbName][collectionName]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "loader = DirectoryLoader('./Texts', glob='./*.txt')\n",
        "data = loader.load()\n",
        "content = {}\n",
        "for doc in data:\n",
        "    content.update({doc.metadata['source'].split('/')[-1].split('.')[0] : doc.page_content.split('.\\n\\n')})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "f1_RamÄ±rez-Duque_ Done\n",
            "fYoung_Behavior Done\n",
            "fPatten_Audio Done\n",
            "fAbbas_2018 Done\n",
            "fQiu Done\n",
            "fzhao2020 Done\n",
            "fcarpenter2020 (1) Done\n",
            "fDawson Done\n",
            "f22_Ouss_ASD Done\n",
            "fLEE Done\n",
            "fAbbas_2020 Done\n"
          ]
        }
      ],
      "source": [
        "for file in content.keys():\n",
        "    print(file, end=' ')\n",
        "    data = []\n",
        "    for text in content[file]:\n",
        "        embed = embeddings.embed_documents([text])\n",
        "        data.append({\"embeddings\":embed, \"text\":text})\n",
        "    collection.insert_many(data)\n",
        "    print('Done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "for doc in collection.find():\n",
        "    arr = doc['embeddings']\n",
        "    flat_arr = np.array(arr).flatten()\n",
        "    query = {'_id':doc['_id']}\n",
        "    update = {\"$set\":{'embeddings':flat_arr.tolist()}}\n",
        "    collection.update_one(query, update)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import pymongo\n",
        "# import datetime\n",
        "\n",
        "# # connect to your Atlas cluster\n",
        "# client = pymongo.MongoClient(\"<connection-string>\")\n",
        "\n",
        "# # define pipeline\n",
        "# pipeline = [\n",
        "#   {\n",
        "#     '$vectorSearch': {\n",
        "#       'index': 'vector-search-tutorial', \n",
        "#       'path': 'plot_embedding', \n",
        "#       'filter': {\n",
        "#         '$or': [\n",
        "#           {\n",
        "#             'genres': {\n",
        "#               '$ne': 'Crime'\n",
        "#             }\n",
        "#           }, {\n",
        "#             '$and': [\n",
        "#               {\n",
        "#                 'year': {\n",
        "#                   '$lte': 2015\n",
        "#                 }\n",
        "#               }, {\n",
        "#                 'genres': {\n",
        "#                   '$eq': 'Action'\n",
        "#                 }\n",
        "#               }\n",
        "#             ]\n",
        "#           }\n",
        "#         ]\n",
        "#       }, \n",
        "#       'queryVector': [],\n",
        "#       'numCandidates': 200, \n",
        "#       'limit': 10\n",
        "#     }\n",
        "#   }, {\n",
        "#     '$project': {\n",
        "#       '_id': 0, \n",
        "#       'title': 1, \n",
        "#       'genres': 1, \n",
        "#       'plot': 1, \n",
        "#       'year': 1, \n",
        "#       'score': {\n",
        "#         '$meta': 'vectorSearchScore'\n",
        "#       }\n",
        "#     }\n",
        "#   }\n",
        "# ]\n",
        "\n",
        "# # run pipeline\n",
        "# result = client[\"sample_mflix\"][\"embedded_movies\"].aggregate(pipeline)\n",
        "\n",
        "# # print results\n",
        "# for i in result:\n",
        "#     print(i)\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "queries = pd.read_csv('Query.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_pipline(query):\n",
        "    queryVector = embeddings.embed_documents([query])\n",
        "    queryVector = np.array(queryVector).flatten()\n",
        "    pipeline = [{\n",
        "        \"$vectorSearch\": {\n",
        "            \"index\": \"vector_index\",\n",
        "            \"path\": \"embeddings\",\n",
        "            \"queryVector\": queryVector.tolist(),\n",
        "            \"numCandidates\": 200,\n",
        "            \"limit\": 10\n",
        "        }\n",
        "    }]\n",
        "    return pipeline\n",
        "\n",
        "def similarity_search(query):\n",
        "    pipeline = make_pipline(query)\n",
        "    result = collection.aggregate(pipeline)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def document_retrival(query):\n",
        "    sample_query = queries.sample(n=1)['Questions '].to_list()[0]\n",
        "    retrived_vectors = similarity_search(sample_query)\n",
        "    list_vectors = [Document(i['text']) for i in retrived_vectors]\n",
        "    return list_vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PublicationsRAG:\n",
        "    def __init__(self, model:str, api_key:str, ) -> None:\n",
        "        self.llm = ChatOpenAI(api_key=api_key, model= model)\n",
        "        self.chain = question_answering.load_qa_chain(llm = self.llm, chain_type=\"map_reduce\")\n",
        "    \n",
        "    def __make_pipline(self, query)->list[dict]:\n",
        "        queryVector = embeddings.embed_documents([query])\n",
        "        queryVector = np.array(queryVector).flatten()\n",
        "        pipeline = [{\n",
        "            \"$vectorSearch\": {\n",
        "                \"index\": \"vector_index\",\n",
        "                \"path\": \"embeddings\",\n",
        "                \"queryVector\": queryVector.tolist(),\n",
        "                \"numCandidates\": 200,\n",
        "                \"limit\": 10\n",
        "            }\n",
        "        }]\n",
        "        return pipeline\n",
        "    \n",
        "    def __similarity_search(self, query)->list[Document]:\n",
        "        pipeline = self.__make_pipline(query)\n",
        "        result = collection.aggregate(pipeline)\n",
        "        list_documents = [Document(i['text']) for i in result]\n",
        "        return list_documents\n",
        "    \n",
        "    def query_run(self, query):\n",
        "        documents = self.__similarity_search(query)\n",
        "        return self.chain.run(input_documents = documents, question = query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "rag = PublicationsRAG('gpt-3.5-turbo', OPENAI_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Various methods to detect atypical patterns of facial expressions in children include:\\n\\n1. Behavioral assessments using structured observations and coding systems.\\n2. Facial expression recognition software utilizing computer-based algorithms.\\n3. Eye-tracking technology to monitor eye movements and gaze patterns.\\n4. Electromyography (EMG) to measure electrical activity in facial muscles.\\n5. Physiological measures like heart rate variability or skin conductance.\\n6. Parent/caregiver reports on the child's facial expressions.\\n7. Multimodal assessments combining different methods for enhanced accuracy.\""
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag.query_run(queries.sample(n=1)['Questions '].to_list()[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The variety of multimodal and multi-modular AI approaches to streamline autism diagnosis in young children include a 4-minute parent-report questionnaire delivered via a mobile app, a list of key behaviors identified from 2-minute, semi-structured home videos of children, and a 2-minute questionnaire presented to the clinician at the time of clinical assessment. Additionally, there is a new module intended for completion in a primary care setting, based on a questionnaire answered by a clinician after examining the child and talking to the parent. These modules are described as fast and easy to administer, providing a streamlined approach to autism diagnosis in young children.\n",
            "Autism Spectrum Disorder (ASD) is a complex neurodevelopmental condition that affects social skills, communication, behavior, and interests. The exact cause of ASD is not fully understood, but research suggests that a combination of genetic and environmental factors may contribute to its development. Genetic factors, including certain genes and mutations, are believed to play a significant role in ASD, as it tends to run in families. Additionally, environmental influences such as advanced parental age, maternal illness during pregnancy, and complications during birth have been associated with an increased risk of developing ASD. It's important to note that there is no single cause of autism, and the interplay of various factors likely contributes to the development of the disorder.\n",
            "There is currently no known cure for Autism Spectrum Disorder (ASD). Treatment typically involves a variety of therapies and interventions aimed at improving the individual's symptoms and quality of life. Early diagnosis and intervention are important in managing ASD effectively.\n",
            "Stereotypical behaviors in Autism Spectrum Disorder (ASD) refer to repetitive movements or actions, such as hand-flapping, rocking, or repeating the same phrases. Maladaptive behaviors in ASD can include aggression, self-injury, or difficulties with emotional regulation. \n",
            "\n",
            "Detection of these behaviors can be done through careful observation by caregivers, educators, or healthcare professionals. Behavioral assessments and standardized scales may also be used to identify and quantify these behaviors. Functional behavior assessments can help determine the underlying reasons for these behaviors. \n",
            "\n",
            "Management typically involves a multidisciplinary approach, including behavioral interventions like Applied Behavior Analysis (ABA), cognitive-behavioral therapy, speech therapy, occupational therapy, and sometimes medication. Individualized intervention plans based on the specific needs and strengths of the individual with ASD are crucial for reducing the frequency and impact of these behaviors and improving overall functioning and quality of life.\n",
            "Eye contact is highly relevant in detecting autism as individuals with autism spectrum disorder (ASD commonly have difficulties with making or maintaining eye contact during social interactions. This behavior is often considered a key indicator in diagnosing autism, as it is linked to challenges in social communication and interaction that are common among individuals with autism. Advanced technologies, such as artificial intelligence systems and computer vision techniques, can analyze eye contact patterns and other facial expressions to help detect potential signs of autism. By combining facial expression data with AI technology, researchers have achieved higher diagnostic accuracy for autism compared to traditional methods. This innovative approach emphasizes the importance of utilizing eye contact and other facial expression data in the early detection and treatment of autism, ultimately aiming to reduce the economic and mental burden on patients and their families.\n",
            "Cross-country trials can be beneficial in the development of machine learning-based multimodal solutions by providing diverse datasets that capture variability across different populations, environments, and conditions. These trials can help in collecting a wide range of audio, video, and other sensory data from various settings, which can then be used to train and evaluate machine learning algorithms. The variability present in cross-country trials can help in improving the robustness and generalizability of the developed models, as they are exposed to different contexts and scenarios. Additionally, cross-country trials can also enable researchers to validate the performance of their models across different cultural, linguistic, and socio-economic backgrounds, ensuring that the developed solutions are effective and applicable in a variety of real-world settings.\n",
            "I don't know.\n",
            "Various methods to detect atypical patterns of facial expressions in children include:\n",
            "\n",
            "1. Observational Assessment by trained professionals during social interactions or play activities.\n",
            "2. Facial Action Coding System (FACS) to anatomically code facial expressions based on muscle movement.\n",
            "3. Computer Vision Analysis (CVA) using technology to detect and track facial landmarks.\n",
            "4. Tablet-Based Behavioral Assessments to capture facial expressions during stimuli engagement.\n",
            "5. Physiological Measurements like electromyography or facial thermography to measure muscle activity or changes in facial temperature.\n",
            "6. Parent or Teacher Reports on a child's facial expressions in different contexts.\n",
            "Facial expressions such as joy, sadness, anger, surprise, fear, disgust, and neutral expressions can be used to detect Autism Disorder in children.\n",
            "Methods to detect Autism from home videos can include using machine learning algorithms to analyze key behaviors displayed by children in the videos. These algorithms can be trained to identify patterns or markers of Autism, such as repetitive behaviors or social communication deficits. Additionally, combining this video analysis with structured parent-reported questionnaires can further enhance the accuracy of the detection process. By merging the results from both sources of information, a more comprehensive assessment can be made for the early detection of Autism in children.\n",
            "The Still-Face Paradigm (SFP) is a method used in early screening for high-risk Autism Spectrum Disorder (ASD) in infants and toddlers. It involves a structured interaction between a caregiver and a child, where the caregiver initially engages with the child normally, then suddenly stops responding emotionally or socially (the \"still-face\" episode), before resuming normal interaction. Social behaviors of the child during these different phases are observed and measured to assess differences between high-risk ASD and typically developing groups. Machine learning methods have been employed to establish models for early ASD screening based on the responses during the SFP.\n",
            "West Syndrome, also known as infantile spasms, is a rare and severe form of epilepsy that typically begins in infancy. It is characterized by clusters of sudden, brief, and uncontrollable muscle contractions or spasms. This condition may lead to developmental regression and various neurological impairments if not treated promptly.\n",
            "The utility of behavior and interaction imaging at 9 months of age in predicting autism spectrum disorder (ASD) and intellectual disability (ID) in high-risk infants with West syndrome (WS) was demonstrated in a study. The best machine learning model achieved 76.47% accuracy classifying WS vs. typically developing controls and 81.25% accuracy classifying WS+ (infants who developed ASD/ID) vs. WS- (infants who did not develop ASD/ID).\n"
          ]
        }
      ],
      "source": [
        "sample_queries = queries['Questions '].to_list()\n",
        "answer = []\n",
        "for query in sample_queries:\n",
        "    ans = rag.query_run(query)\n",
        "    answer.append(ans)\n",
        "    print(ans)\n",
        "\n",
        "queries['answer'] = answer\n",
        "queries.to_csv('Answers.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
